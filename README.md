# Fine-Tuned DeBERTa for Zero-Shot Classification
This project fine-tunes the pre-trained large language model (LLM) MoritzLaurer/deberta-v3-large-zeroshot-v1, a transformer-based model designed for zero-shot learning, on a custom dataset (training.csv). The goal is to enhance its performance for a specific classification task (classify text into 4 different Quantum Computing labels). By training on labeled examples, the model leverages its pre-existing knowledge to adapt to the target domain, effectively capturing contextual relationships in text while maintaining strong generalization, even with limited data.

## Project details
* [Code Link](https://github.com/johnny880624/Classification_through_fine_tuning/blob/main/Kuan_Lin.ipynb)
* [Detailed Report](https://github.com/johnny880624/Classification_through_fine_tuning/blob/main/Kuan_Lin_report.pdf)
* [Training Dataset](https://github.com/johnny880624/Classification_through_fine_tuning/blob/main/Datasets.zip)
* [Classification Output](https://github.com/johnny880624/Classification_through_fine_tuning/blob/main/Kuan_Lin_testing.csv)
